{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "train_y = train[[\"SALE PRICE\"]]\n",
    "train_X = train\n",
    "del train_X[\"SALE PRICE\"]\n",
    "\n",
    "test_X = pd.read_csv(\"./data/test.csv\")\n",
    "test_y = pd.read_csv(\"./data/test_groundtruth.csv\")\n",
    "\n",
    "print(\"train_X:\",train_X.shape)\n",
    "print(\"train_y:\",train_y.shape)\n",
    "print(\"test_X:\",test_X.shape)\n",
    "print(\"test_y:\",test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = len(train_X)\n",
    "num_test_samples=len(test_X)\n",
    "data_X = pd.concat([train_X, test_X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's delete some of the columns that we ** may not ** need\n",
    "# 请注意 下面删除的特征很可能是有用的，合理的处理能够获得更为准确的预测模型，请探索所删除特征的使用\n",
    "del data_X['ADDRESS']\n",
    "del data_X['APARTMENT NUMBER']\n",
    "#del data_X['BLOCK']\n",
    "#del data_X['LOT']\n",
    "#del data_X['BUILDING CLASS AT PRESENT']\n",
    "#del data_X['BUILDING CLASS AT TIME OF SALE']\n",
    "#del data_X['NEIGHBORHOOD']\n",
    "#del data_X['SALE DATE']\n",
    "#del data_X['LAND SQUARE FEET']\n",
    "#del data_X['GROSS SQUARE FEET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = data_X.isnull().sum()\n",
    "aa[aa>0].sort_values(ascending=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's convert some of the columns to appropriate datatype\n",
    "\n",
    "data_X['TAX CLASS AT TIME OF SALE'] = data_X['TAX CLASS AT TIME OF SALE'].astype('category')\n",
    "data_X['TAX CLASS AT PRESENT'] = data_X['TAX CLASS AT PRESENT'].astype('category')\n",
    "data_X['BUILDING CLASS AT TIME OF SALE'] = data_X['BUILDING CLASS AT TIME OF SALE'].astype('category')\n",
    "data_X['BUILDING CLASS AT PRESENT'] = data_X['BUILDING CLASS AT PRESENT'].astype('category')\n",
    "#data_X['APARTMENT NUMBER'] = data_X['APARTMENT NUMBER'].astype('category')\n",
    "data_X['BOROUGH'] = data_X['BOROUGH'].astype('category')\n",
    "data_X['SALE DATE']=data_X['SALE DATE'].astype(str)\n",
    "data_X['oSALE DATE']=data_X['SALE DATE'].astype(str)\n",
    "data_X.replace(' -  ',0,inplace=True)\n",
    "data_X['GROSS SQUARE FEET']=data_X['GROSS SQUARE FEET'].astype(\"int64\")\n",
    "data_X['LAND SQUARE FEET']=data_X['LAND SQUARE FEET'].astype(\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_X[\"oTAX CLASS AT TIME OF SALE\"]=data_X[\"TAX CLASS AT TIME OF SALE\"].map({1:58,2:68,4:99})\n",
    "data_X[\"oTAX CLASS AT PRESENT\"]=data_X[\"TAX CLASS AT PRESENT\"].map({\"1\":59,\"1A\":40,\"1B\":40,\"1C\":95,\"2\":63,\"2A\":110,\"2B\":200,\"2C\":85,\"4\":95,\" \":95})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X_train=data_X[:num_train_samples]\n",
    "data_X_test=data_X[num_train_samples:]\n",
    "for i in range(num_train_samples):\n",
    "    tmp=data_X_train['SALE DATE'][i].split('-')\n",
    "    data_X_train['oSALE DATE'][i]=(int(tmp[0])-2016)*12+int(tmp[1])\n",
    "    #data_X_train['ADDRESS'][i]=data_X_train[\"ADDRESS\"][i].split(',')[0]\n",
    "for i in range ( num_test_samples ):\n",
    "    tmp=data_X_test['SALE DATE'][i].split('-')\n",
    "    data_X_test['oSALE DATE'][i]=(int(tmp[0])-2016)*12+int(tmp[1])\n",
    "    #data_X_test['ADDRESS'][i]=data_X_test[\"ADDRESS\"][i].split(',')[0]\n",
    "data_X = pd.concat([data_X_train, data_X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X['NEIGHBORHOOD'] = data_X['NEIGHBORHOOD'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the variables to be one-hot encoded\n",
    "one_hot_features = ['BOROUGH', 'BUILDING CLASS CATEGORY','TAX CLASS AT PRESENT','TAX CLASS AT TIME OF SALE','BUILDING CLASS AT PRESENT','BUILDING CLASS AT TIME OF SALE','SALE DATE','NEIGHBORHOOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables into dummy/indicator variables (i.e. one-hot encoding).\n",
    "one_hot_encoded = pd.get_dummies(data_X[one_hot_features])\n",
    "one_hot_encoded.info(verbose=True, memory_usage=True, null_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = data_X.drop(one_hot_features,axis=1)\n",
    "data_X = pd.concat([data_X, one_hot_encoded] ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data_X[:num_train_samples].to_numpy()\n",
    "test_X = data_X[num_train_samples:].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33963923237201954"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_regr = RandomForestRegressor()\n",
    "rf_regr.fit(train_X, train_y)\n",
    "Y_pred_rf = rf_regr.predict(test_X)\n",
    "\n",
    "# MAPE metric\n",
    "mean_absolute_percentage_error(test_y,Y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "scaler = RobustScaler()  # 对数据进行缩放\n",
    "n_train=train.shape[0] #获得有多少条训练数据\n",
    "#del data_X[\"NEIGHBORHOOD\"]\n",
    "#del data_X[\"ADDRESS\"]\n",
    "#del data_X[\"SALE DATE\"]\n",
    "X = data_X[:n_train] # 获得训练数据\n",
    "test_X = data_X[n_train:] # 获得测试数据\n",
    "y= test_y\n",
    "\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "y_log = np.log(y) # 对于 y 进行人为定义log 进行缩放 \n",
    "test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_logg=np.log(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  2.6min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31791926117269637"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_regr = RandomForestRegressor(verbose=True)\n",
    "rf_regr.fit(X_scaled,y_logg)\n",
    "Y_pred_rf = rf_regr.predict(test_X_scaled)\n",
    "\n",
    "# MAPE metric\n",
    "mean_absolute_percentage_error(test_y,np.exp(Y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=1))  # 5折交叉验证\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge: 537623.427874, 8893.4063\n",
      "LinSVR: 707419.379389, 22409.0992\n"
     ]
    }
   ],
   "source": [
    "models = [Ridge(),Lasso(alpha=0.01,max_iter=10000),RandomForestRegressor(), GradientBoostingRegressor(),  LinearSVR(),\n",
    "          ElasticNet(alpha=0.001,max_iter=10000), BayesianRidge(), KernelRidge (alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n",
    "          ExtraTreesRegressor(),XGBRegressor()] \n",
    "models=[Ridge(), LinearSVR(), BayesianRidge(),ExtraTreesRegressor(),XGBRegressor()]\n",
    "names = [\"Ridge\", \"LinSVR\", \"Bay\",\"Extra\",\"Xgb\"]\n",
    "for name, model in zip(names, models):\n",
    "    score = rmse_cv(model, X_scaled, y_logg)\n",
    "    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.impute import SimpleImputer as Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageWeight(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,mod,weight):\n",
    "        self.mod = mod\n",
    "        self.weight = weight\n",
    "        self.nums=len(mod)\n",
    "    def fit(self,X,y):\n",
    "        self.models_ = [clone(x) for x in self.mod]\n",
    "        for model in self.models_:\n",
    "            model.fit(X,y)\n",
    "            print(\"完成！\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X,models,weight):\n",
    "        self.weight = weight\n",
    "        u = X.shape[0]\n",
    "        w=[]\n",
    "        for i in range(u):\n",
    "            w.append(0)\n",
    "        for i in models:\n",
    "            print(\"基础\")\n",
    "            tmp=self.models_[i].predict(X)*self.weight[i]\n",
    "            for j in range(u):\n",
    "                w[j]=w[j]+tmp[j]\n",
    "        #pred = np.array([model.predict(X) for model in self.models_])\n",
    "        # for every data point, single model prediction times weight, then add them together\n",
    "        #for data in range(pred.shape[1]):\n",
    "         #   single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n",
    "          #  w.append(np.sum(single))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.001,max_iter=10000)\n",
    "ridge = Ridge()\n",
    "RF = RandomForestRegressor()\n",
    "GBR = GradientBoostingRegressor()\n",
    "LinSVR = LinearSVR()\n",
    "Ela =  ElasticNet(alpha=0.001,max_iter=10000)\n",
    "Bay =BayesianRidge() \n",
    "Extra =  ExtraTreesRegressor()\n",
    "Xgb = XGBRegressor(max_depth=6,n_estimators=800,learning_rate=0.16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.2\n",
    "w2 = 0.2\n",
    "w3 = 0.6\n",
    "w4 = 0.3\n",
    "w5 = 0.05\n",
    "w6 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成！\n",
      "完成！\n",
      "完成！\n",
      "完成！\n",
      "完成！\n",
      "完成！\n",
      "完成！\n",
      "完成！\n",
      "完成！\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AverageWeight(mod=[Lasso(alpha=0.001, max_iter=10000), Ridge(),\n",
       "                   RandomForestRegressor(), GradientBoostingRegressor(),\n",
       "                   LinearSVR(), ElasticNet(alpha=0.001, max_iter=10000),\n",
       "                   BayesianRidge(), ExtraTreesRegressor(),\n",
       "                   XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                                colsample_bylevel=None, colsample_bynode=None,\n",
       "                                colsample_bytree=None,\n",
       "                                early_stopping_rounds=No...\n",
       "                                grow_policy=None, importance_type=None,\n",
       "                                interaction_constraints=None,\n",
       "                                learning_rate=None, max_bin=None,\n",
       "                                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                max_delta_step=None, max_depth=None,\n",
       "                                max_leaves=None, min_child_weight=None,\n",
       "                                missing=nan, monotone_constraints=None,\n",
       "                                n_estimators=100, n_jobs=None,\n",
       "                                num_parallel_tree=None, predictor=None,\n",
       "                                random_state=None, ...)],\n",
       "              weight=[0.2, 0.2, 0.6])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AverageWeight</label><div class=\"sk-toggleable__content\"><pre>AverageWeight(mod=[Lasso(alpha=0.001, max_iter=10000), Ridge(),\n",
       "                   RandomForestRegressor(), GradientBoostingRegressor(),\n",
       "                   LinearSVR(), ElasticNet(alpha=0.001, max_iter=10000),\n",
       "                   BayesianRidge(), ExtraTreesRegressor(),\n",
       "                   XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                                colsample_bylevel=None, colsample_bynode=None,\n",
       "                                colsample_bytree=None,\n",
       "                                early_stopping_rounds=No...\n",
       "                                grow_policy=None, importance_type=None,\n",
       "                                interaction_constraints=None,\n",
       "                                learning_rate=None, max_bin=None,\n",
       "                                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                max_delta_step=None, max_depth=None,\n",
       "                                max_leaves=None, min_child_weight=None,\n",
       "                                missing=nan, monotone_constraints=None,\n",
       "                                n_estimators=100, n_jobs=None,\n",
       "                                num_parallel_tree=None, predictor=None,\n",
       "                                random_state=None, ...)],\n",
       "              weight=[0.2, 0.2, 0.6])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AverageWeight(mod=[Lasso(alpha=0.001, max_iter=10000), Ridge(),\n",
       "                   RandomForestRegressor(), GradientBoostingRegressor(),\n",
       "                   LinearSVR(), ElasticNet(alpha=0.001, max_iter=10000),\n",
       "                   BayesianRidge(), ExtraTreesRegressor(),\n",
       "                   XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "                                colsample_bylevel=None, colsample_bynode=None,\n",
       "                                colsample_bytree=None,\n",
       "                                early_stopping_rounds=No...\n",
       "                                grow_policy=None, importance_type=None,\n",
       "                                interaction_constraints=None,\n",
       "                                learning_rate=None, max_bin=None,\n",
       "                                max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "                                max_delta_step=None, max_depth=None,\n",
       "                                max_leaves=None, min_child_weight=None,\n",
       "                                missing=nan, monotone_constraints=None,\n",
       "                                n_estimators=100, n_jobs=None,\n",
       "                                num_parallel_tree=None, predictor=None,\n",
       "                                random_state=None, ...)],\n",
       "              weight=[0.2, 0.2, 0.6])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_avg = AverageWeight(mod = [lasso,ridge,RF,GBR,LinSVR,Ela,Bay,Extra,Xgb],weight=[w1,w2,w3])\n",
    "weight_avg.fit(X_scaled,y_logg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(Self,X,models,weight):\n",
    "    Self.weight = weight\n",
    "    u = X.shape[0]\n",
    "    w=[]\n",
    "    for i in range(u):\n",
    "        w.append(0)\n",
    "    for i in range(len(models)):\n",
    "        print(\"基础\")\n",
    "        tmp=Self.models_[models[i]].predict(X)*weight[i]\n",
    "        for j in range(u):\n",
    "            w[j]=w[j]+tmp[j]\n",
    "        #pred = np.array([model.predict(X) for model in self.models_])\n",
    "        # for every data point, single model prediction times weight, then add them together\n",
    "        #for data in range(pred.shape[1]):\n",
    "         #   single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n",
    "          #  w.append(np.sum(single))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础\n",
      "基础\n",
      "基础\n",
      "基础\n",
      "基础\n",
      "基础\n",
      "基础\n",
      "基础\n",
      "基础\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40257969217595857,\n",
       " 0.37496789140252884,\n",
       " 0.31799019183544625,\n",
       " 0.38904074412066963,\n",
       " 0.371064836879462,\n",
       " 0.3901713394313403,\n",
       " 0.3749617099592213,\n",
       " 0.33806137893260235,\n",
       " 0.3412323535932328]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W=[]\n",
    "for i in range(weight_avg.nums):\n",
    "            print(\"基础\")\n",
    "            tmp=weight_avg.models_[i].predict(test_X_scaled)\n",
    "            W.append(mean_absolute_percentage_error(test_y,np.exp(tmp)))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础\n",
      "基础\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3177509369177875"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred_rf = predict(weight_avg,test_X_scaled,[2,7],[0.8,0.2])\n",
    "mean_absolute_percentage_error(test_y,np.exp(Y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_avg = AverageWeight(mod = [Extra, ridge, Xgb, RF,GBR, Bay],weight=[w1,w2,w3,w4,w5,w6])\n",
    "rmse_cv(weight_avg,X_scaled,y_logg),  rmse_cv(weight_avg,X_scaled,y_logg).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_regr.fit(X_scaled, y_logg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31875050389417536"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "Y_pred_rf = rf_regr.predict(test_X_scaled)\n",
    "mean_absolute_percentage_error(test_y,np.exp(Y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regr.fit(X_scaled, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33900199335374487"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y_pred_rf = rf_regr.predict(test_X_scaled)\n",
    "mean_absolute_percentage_error(test_y,Y_pred_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"pred\":Y_pred_rf}).to_csv(\"学号_姓名.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 800 0.1 0.3303402501199265\n",
      "totally time is 0:02:46.693772\n",
      "6 800 0.15 0.32683184550392264\n",
      "totally time is 0:02:46.436442\n",
      "6 800 0.2 0.32766503871163666\n",
      "totally time is 0:02:45.732962\n",
      "6 1200 0.1 0.3277392439911786\n",
      "totally time is 0:04:09.835459\n",
      "6 1200 0.15 0.32629573384049854\n",
      "totally time is 0:04:08.190134\n",
      "6 1200 0.2 0.328235370728801\n",
      "totally time is 0:04:07.282323\n",
      "6 1600 0.1 0.3265961848798322\n",
      "totally time is 0:05:32.041322\n",
      "6 1600 0.15 0.32719867177775813\n",
      "totally time is 0:05:30.230619\n",
      "6 1600 0.2 0.3310832375424873\n",
      "totally time is 0:05:29.484162\n",
      "6 2400 0.1 0.3273328511501888\n",
      "totally time is 0:08:15.320929\n",
      "6 2400 0.15 0.3291598506620614\n",
      "totally time is 0:08:13.284119\n",
      "6 2400 0.2 0.3372376604996416\n",
      "totally time is 0:08:11.234391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3303402501199265,\n",
       " 0.32683184550392264,\n",
       " 0.32766503871163666,\n",
       " 0.3277392439911786,\n",
       " 0.32629573384049854,\n",
       " 0.328235370728801,\n",
       " 0.3265961848798322,\n",
       " 0.32719867177775813,\n",
       " 0.3310832375424873,\n",
       " 0.3273328511501888,\n",
       " 0.3291598506620614,\n",
       " 0.3372376604996416]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "WWW=[]\n",
    "depth=[6]\n",
    "trees=[800,1200,1600,2400]\n",
    "lr=[0.1,0.15,0.2]\n",
    "for de in depth:\n",
    "    for tree in trees:\n",
    "        for lrr in lr:\n",
    "            start = datetime.datetime.now()\n",
    "            mm=XGBRegressor(max_depth=de,n_estimators=tree,learning_rate=lrr)        # 使用多少棵树来拟合，也可以理解为多少次迭代。默认100；\n",
    "            mm.fit(X_scaled,y_logg)\n",
    "            YY=mm.predict(test_X_scaled)\n",
    "            tmp=mean_absolute_percentage_error(test_y,np.exp(YY))\n",
    "            end = datetime.datetime.now()\n",
    "            print(de,end=\" \")\n",
    "            print(tree,end=\" \")\n",
    "            print(lrr,end=\" \")\n",
    "            print(tmp)\n",
    "            print('totally time is ' ,end = \"\")\n",
    "            print(end-start)\n",
    "            WWW.append(tmp)\n",
    "WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 800 0.05 0.3350308184883919\n",
      "totally time is 0:01:56.563457\n",
      "6 800 0.1 0.3303402501199265\n",
      "totally time is 0:02:03.939198\n",
      "6 800 0.15 0.32683184550392264\n",
      "totally time is 0:02:07.982595\n",
      "6 800 0.2 0.32766503871163666\n",
      "totally time is 0:02:08.521636\n",
      "6 800 0.4 0.33831505484808144\n",
      "totally time is 0:02:08.446361\n",
      "6 1200 0.05 0.33193211481385765\n",
      "totally time is 0:03:17.188579\n",
      "6 1200 0.1 0.3277392439911786\n",
      "totally time is 0:03:16.967780\n",
      "6 1200 0.15 0.32629573384049854\n",
      "totally time is 0:03:19.211351\n",
      "6 1200 0.2 0.328235370728801\n",
      "totally time is 0:03:18.798700\n",
      "6 1200 0.4 0.3443793738692462\n",
      "totally time is 0:03:17.496033\n",
      "6 1600 0.05 0.329758949381\n",
      "totally time is 0:04:24.511116\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "WWW=[]\n",
    "depth=[6]\n",
    "trees=[800,1200,1600]\n",
    "lr=[0.1,0.15,0.2]\n",
    "for de in depth:\n",
    "    for tree in trees:\n",
    "        for lrr in lr:\n",
    "            start = datetime.datetime.now()\n",
    "            mm=XGBRegressor(max_depth=de,n_estimators=tree,learning_rate=lrr)        # 使用多少棵树来拟合，也可以理解为多少次迭代。默认100；\n",
    "            mm.fit(X_scaled,y_logg)\n",
    "            YY=mm.predict(test_X_scaled)\n",
    "            tmp=mean_absolute_percentage_error(test_y,np.exp(YY))\n",
    "            end = datetime.datetime.now()\n",
    "            print(de,end=\" \")\n",
    "            print(tree,end=\" \")\n",
    "            print(lrr,end=\" \")\n",
    "            print(tmp)\n",
    "            print('totally time is ' ,end = \"\")\n",
    "            print(end-start)\n",
    "            WWW.append(tmp)\n",
    "WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 25.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 800 500 0.3169416656576877\n",
      "totally time is 0:25:27.676281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3169416656576877]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "WWW=[]\n",
    "depth=[6]\n",
    "trees=[800]\n",
    "lr=[500]\n",
    "for de in depth:\n",
    "    for tree in trees:\n",
    "        for lrr in lr:\n",
    "            start = datetime.datetime.now()\n",
    "            mm=RandomForestRegressor(n_estimators=lrr,verbose=True)        # 使用多少棵树来拟合，也可以理解为多少次迭代。默认100；\n",
    "            mm.fit(X_scaled,y_logg)\n",
    "            YY=mm.predict(test_X_scaled)\n",
    "            tmp=mean_absolute_percentage_error(test_y,np.exp(YY))\n",
    "            end = datetime.datetime.now()\n",
    "            print(de,end=\" \")\n",
    "            print(tree,end=\" \")\n",
    "            print(lrr,end=\" \")\n",
    "            print(tmp)\n",
    "            print('totally time is ' ,end = \"\")\n",
    "            print(end-start)\n",
    "            WWW.append(tmp)\n",
    "WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 3200 0.1 0.32946608987565384\n",
      "totally time is 0:11:16.280694\n",
      "6 3200 0.15 0.33263571701142064\n",
      "totally time is 0:11:12.309581\n",
      "6 3200 0.2 0.34177348246326283\n",
      "totally time is 0:11:07.716917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32946608987565384, 0.33263571701142064, 0.34177348246326283]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "WWW=[]\n",
    "depth=[6]\n",
    "trees=[3200]\n",
    "lr=[0.1,0.15,0.2]\n",
    "for de in depth:\n",
    "    for tree in trees:\n",
    "        for lrr in lr:\n",
    "            start = datetime.datetime.now()\n",
    "            mm=XGBRegressor(max_depth=de,n_estimators=tree,learning_rate=lrr)        # 使用多少棵树来拟合，也可以理解为多少次迭代。默认100；\n",
    "            mm.fit(X_scaled,y_logg)\n",
    "            YY=mm.predict(test_X_scaled)\n",
    "            tmp=mean_absolute_percentage_error(test_y,np.exp(YY))\n",
    "            end = datetime.datetime.now()\n",
    "            print(de,end=\" \")\n",
    "            print(tree,end=\" \")\n",
    "            print(lrr,end=\" \")\n",
    "            print(tmp)\n",
    "            print('totally time is ' ,end = \"\")\n",
    "            print(end-start)\n",
    "            WWW.append(tmp)\n",
    "WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 160 0.1 0.33785220544221456\n",
      "totally time is 0:06:52.035447\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33785220544221456]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "WWW=[]\n",
    "depth=[160]\n",
    "trees=[160]\n",
    "lr=[0.1]\n",
    "for de in depth:\n",
    "    for tree in trees:\n",
    "        for lrr in lr:\n",
    "            start = datetime.datetime.now()\n",
    "            mm=ExtraTreesRegressor(max_depth=de,n_estimators=tree)        # 使用多少棵树来拟合，也可以理解为多少次迭代。默认100；\n",
    "            mm.fit(X_scaled,y_logg)\n",
    "            YY=mm.predict(test_X_scaled)\n",
    "            tmp=mean_absolute_percentage_error(test_y,np.exp(YY))\n",
    "            end = datetime.datetime.now()\n",
    "            print(de,end=\" \")\n",
    "            print(tree,end=\" \")\n",
    "            print(lrr,end=\" \")\n",
    "            print(tmp)\n",
    "            print('totally time is ' ,end = \"\")\n",
    "            print(end-start)\n",
    "            WWW.append(tmp)\n",
    "WWW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
