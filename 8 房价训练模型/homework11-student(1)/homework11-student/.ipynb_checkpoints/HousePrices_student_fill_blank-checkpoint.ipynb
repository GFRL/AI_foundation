{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Exploratory Visualization__  \n",
    "__2. Data Cleaning__  \n",
    "__3. Feature Engineering__  \n",
    "__4. Modeling & Evaluation__  \n",
    "__5. Ensemble Methods__  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HIGHLIGHT: ... 的部分是需要补全的部分, 请根据代码和注释对相应的部分进行补全 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载数据处理分析，可视化等实验过程中可能用到的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "## 忽视掉 一些 warning 提醒\n",
    "warnings.filterwarnings('ignore')\n",
    "## 设置可视化的风格 \n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载机器学习相关的库\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from scipy.stats import skew\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.preprocessing import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 加载机器学习相关的库\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import ElasticNet, SGDRegressor, BayesianRidge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 设置 panda 展示数据时的相关参数\n",
    "pd.set_option('max_colwidth',200)\n",
    "pd.set_option('display.width',200)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填写数据的路径并读取\n",
    "train_path = ...\n",
    "test_path = ...\n",
    "train=pd.read_csv(...)\n",
    "test=pd.read_csv(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单看下数据\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train的属性有 YearBuilt, SalePrice，可以用 train.YearBuilt 和 train.SalePrice 来获取\n",
    "# 新住宅是否比老住宅均价更高？\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.boxplot(..., ...)  # 可以按照年份画出价格的箱线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(..., ...) # 可以按照年份画出价格的线性图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __好像年份也是和价格有关系的，后续可以把年份也作为一项特征.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIGHLIGHT: 在数据清洗阶段注重理解背后的逻辑，我们会在删除和填充等做法各举一个例子，然后引导大家分析其他的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看训练集数据的其他属性比如 GrLivArea 和 SalePrice 的关系\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.scatter(x=..., y=...) # 可以按照GrLivArea的大小来画出价格的散点图\n",
    "plt.xlabel(\"GrLivArea\", fontsize=13)\n",
    "plt.ylabel(\"SalePrice\", fontsize=13)\n",
    "plt.ylim(0,800000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __As is shown in the kernels above, the bottom right two two points with extremely large GrLivArea are likely to be outliers. So we delete them.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GrLivArea字段过大的值可能是离群点, 可以通过删除训练样本的方式处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 丢掉 GrLivArea 大于 4000 且 SalePrice 小于 300000 的点\n",
    "train.drop(train[(train[\"...\"]>...)&(train[\"...\"]<...)].index,inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初步了解数据后，我们把训练数据和测试数据拼在一起方便后续处理\n",
    "# 注意 训练集在前\n",
    "full=pd.concat([..., ...], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 去掉 id 这一列的属性 查看数据的形状\n",
    "full.drop(['...'],axis=1, inplace=True)\n",
    "full.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 查看数据是否有缺失\n",
    "aa = full.isnull().sum()\n",
    "aa[aa>0].sort_values(ascending=False)  \n",
    "\n",
    "# 我们将对缺失的数据进行补充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于部分字段的缺失值, 可以通过填充中位数处理, 例如 “LotFrontage” \n",
    "# Let's first imput the missing values of LotFrontage based on the median of LotArea and Neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们对整个数据根据 Neighborhood 进行聚类, 并 计算 LotFrontage  均值 (mean), 中值 (media), 和数目 \n",
    "# 并查看结果\n",
    " \n",
    "full.groupby(['Neighborhood'])[['...']].agg(['...','...','count']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # 由于LotArea是一个连续的特性, 我们使用qcut函数将其划分为10个部分\n",
    "\n",
    "full[\"LotAreaCut\"] = pd.qcut(full.LotArea, q=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们根据 LotAreaCut 对数据进行聚类，计算 LotFrontage 均值, 中值, 和 数目\n",
    "# 并查看结果\n",
    "full.groupby(['...'])[['LotFrontage']].agg(['mean','median','...']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据 LotAreaCut 和 Neighborhood 对数据进行聚类, 对 LotFrontage 空的值填充 LotFrontage 的中值 \n",
    "\n",
    "full['LotFrontage']=full.groupby(['...','...'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于LotArea和Neighborhood的某些组合不可用, 因此我们仅使用LotAreaCut来进行聚类, 对LotFrontage填充 LotFrontage 的 中值\n",
    "\n",
    "full['LotFrontage']=full.groupby(['...'])['LotFrontage'].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __Then we filling in other missing values according to data_description.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果我们仔细观察data_description里面的内容，就会发现很多缺失值都有迹可寻，比如PoolQC, 表示游泳池的质量，如果这个值缺失, 就代表这套房子没有游泳池，因此用 None 来填补\n",
    "cols1 = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\n",
    "for col in cols1:\n",
    "    full[col].fillna(\"...\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 下面这些多为表示XX面积，比如 TotalBsmtSF 表示的是地下室面积，如果一个房间，没有地下室，那就可以使用 0 来填充\n",
    "cols=[\"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"GarageCars\", \"BsmtFinSF2\", \"BsmtFinSF1\", \"GarageArea\"]\n",
    "for col in cols:\n",
    "    full[col].fillna(..., inplace=True)\n",
    "\n",
    "# fill in with mode\n",
    "cols2 = [\"MSZoning\", \"BsmtFullBath\", \"BsmtHalfBath\", \"Utilities\", \"Functional\", \"Electrical\", \"KitchenQual\", \"SaleType\",\"Exterior1st\", \"Exterior2nd\"]\n",
    "for col in cols2:\n",
    "    full[col].fillna(full[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __And there is no missing data except for the value we want to predict !__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 检查是否还有其他需要填充的数据呢？\n",
    "\n",
    "full.isnull().sum()[full.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部分类别特征, 可以通过映射成数值特征来完成转换, 如下所示\n",
    "# 换句话说，我们可以通过设置一个映射函数, 把部分类别特征转化为数值特征, 这样可以很方便后续我们进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是相关的类别特征 (通常会以字符串记录), 所以我们将其转化为字符串类型\n",
    "NumStr = [\"MSSubClass\",\"BsmtFullBath\",\"BsmtHalfBath\",\"HalfBath\",\"BedroomAbvGr\",\"KitchenAbvGr\",\"MoSold\",\"YrSold\",\"YearBuilt\",\"YearRemodAdd\",\"LowQualFinSF\",\"GarageYrBlt\"]\n",
    "for col in NumStr:\n",
    "    full[col]=full[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 问题就转化为了如何构造这个映射函数呢？ \n",
    "# 我们可以根据一个特性按SalePrice分组, 并根据平均值和中值对其进行排序\n",
    "# 以 MSSubClass 为例, 我们根据 MSSubClass 进行聚类，计算 SalePrice 的统计均值, 中位数等信息\n",
    "full.groupby(['...'])[['SalePrice']].agg(['mean','median','count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __根据上面的计算结果，我们设计了如下的映射函数__  \n",
    "\n",
    "                '180' : 1\n",
    "                '30' : 2   '45' : 2\n",
    "                '190' : 3, '50' : 3, '90' : 3,\n",
    "                '85' : 4, '40' : 4, '160' : 4\n",
    "                '70' : 5, '20' : 5, '75' : 5, '80' : 5, '150' : 5\n",
    "                '120': 6, '60' : 6\n",
    "\n",
    "+ __你也可以根据你自己的直觉和偏好来设计出你自己的映射函数，但是必须符合逻辑，否则模型很难从这一特征中学到有价值的信息__  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到映射函数, 我们用代码实现这个构造函数\n",
    "full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n",
    "                                        '30':2, '45':2, \n",
    "                                        '190':3, '50':3, '90':3, \n",
    "                                        '85':4, '40':4, '160':4, \n",
    "                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n",
    "                                        '120': 6, '60':6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __我们可以参考上述过程继续构造下面的映射函数，我还在特性前面添加了一个小“o”，以便保留原始特性，以便稍后使用 get_dummies 编码函数.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 鼓励大家尝试参考上述方案构造映射函数，填空的部分较为简单\n",
    "### 我也把参考的映射函数放在代码的最后\n",
    "\n",
    "def map_values():\n",
    "    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n",
    "                                        '30':2, '45':2, \n",
    "                                        '190':3, '50':3, '90':3, \n",
    "                                        '85':4, '40':4, '160':4, \n",
    "                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n",
    "                                        '120': 6, '60':6})\n",
    "    \n",
    "    full[\"oMSZoning\"] = full.MSZoning.map(...)\n",
    "    \n",
    "    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n",
    "                                               'IDOTRR':2, 'BrDale':2,\n",
    "                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n",
    "                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n",
    "                                               'NPkVill':5, 'Mitchel':5,\n",
    "                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n",
    "                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n",
    "                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n",
    "                                               'StoneBr':9,\n",
    "                                               'NoRidge':10, 'NridgHt':10})\n",
    "    \n",
    "    full[\"oCondition1\"] = full.Condition1.map(...)\n",
    "    \n",
    "    full[\"oBldgType\"] = full.BldgType.map(...)\n",
    "    \n",
    "    full[\"oHouseStyle\"] = full.HouseStyle.map(...)\n",
    "    \n",
    "    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n",
    "                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n",
    "                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n",
    "                                             'BrkFace':4, 'Plywood':4,\n",
    "                                             'VinylSd':5,\n",
    "                                             'CemntBd':6,\n",
    "                                             'Stone':7, 'ImStucc':7})\n",
    "    \n",
    "    full[\"oMasVnrType\"] = full.MasVnrType.map(...)\n",
    "    \n",
    "    full[\"oExterQual\"] = full.ExterQual.map(...)\n",
    "    \n",
    "    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n",
    "                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n",
    "                                           'Wood':3, 'PConc':4})\n",
    "    \n",
    "    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oBsmtExposure\"] = full.BsmtExposure.map(...)\n",
    "    \n",
    "    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n",
    "    \n",
    "    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oKitchenQual\"] = full.KitchenQual.map(...)\n",
    "    \n",
    "    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n",
    "    \n",
    "    full[\"oFireplaceQu\"] = full.FireplaceQu.map(...)\n",
    "    \n",
    "    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n",
    "                                           'Detchd':2,\n",
    "                                           '2Types':3, 'Basment':3,\n",
    "                                           'Attchd':4, 'BuiltIn':5})\n",
    "    \n",
    "    full[\"oGarageFinish\"] = full.GarageFinish.map(...)\n",
    "    \n",
    "    full[\"oPavedDrive\"] = full.PavedDrive.map(...)\n",
    "    \n",
    "    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n",
    "                                       'CWD':2, 'Con':3, 'New':3})\n",
    "    \n",
    "    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n",
    "                \n",
    "                        \n",
    "    return \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去掉 LotAreaCut 和 SalePrice 这两个 unwanted value\n",
    "full.drop(\"LotAreaCut\",axis=1,inplace=True)\n",
    "full.drop(['SalePrice'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __接下来我们可以建立一个管道。 拥有管道后，可以很方便地试验不同的功能组合.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照我们最开始的观察，year 有关的变量是和价格有关系的，因此我们需要对三个和 Year 有关的变量进行编码\n",
    "class labelenc(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        lab=LabelEncoder()\n",
    "        X[\"YearBuilt\"] = lab.fit_transform(X[\"YearBuilt\"])\n",
    "        X[\"YearRemodAdd\"] = lab.fit_transform(X[\"YearRemodAdd\"])\n",
    "        X[\"GarageYrBlt\"] = lab.fit_transform(X[\"GarageYrBlt\"])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 抽取特征的斜度, 对于符合斜度要求的特征应用 log1p, 然后使用 get_dummies函数 进行独热编码\n",
    "class skew_dummies(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,skew=0.5):\n",
    "        self.skew = skew\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X_numeric=X.select_dtypes(exclude=[\"object\"])\n",
    "        skewness = X_numeric.apply(lambda x: skew(x)) \n",
    "        \"\"\"\n",
    "        Examples\n",
    "        skew(x) Compute the skewness of a data set.\n",
    "        For normally distributed data, the skewness should be about 0. For\n",
    "        unimodal continuous distributions, a skewness value > 0 means that\n",
    "        there is more weight in the right tail of the distribution.\n",
    "        --------\n",
    "        >>> from scipy.stats import skew\n",
    "        >>> skew([1, 2, 3, 4, 5])\n",
    "        0.0\n",
    "        >>> skew([2, 8, 0, 4, 1, 9, 9, 0])\n",
    "        0.2650554122698573\n",
    "        \"\"\"\n",
    "        skewness_features = skewness[abs(skewness) >= self.skew].index\n",
    "        X[skewness_features] = np.log1p(X[skewness_features])\n",
    "        X = pd.get_dummies(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "pipe = Pipeline([\n",
    "    ('labenc', labelenc()),\n",
    "    ('skew_dummies', skew_dummies(skew=1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the original data for later use\n",
    "full2 = full.copy()\n",
    "data_pipe = pipe.fit_transform(full2)\n",
    "data_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 你能理清楚为什么 full.shape 是怎样一步一步变化到 data_pipe的shape的吗？\n",
    "# 可以画一个流程图，帮助自己理解整个过程\n",
    "# (i,j)-> action -> (m,n) -> action ->...... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe.head() # 展示头几个数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __因为咱们的数据里面还有其他异常值，所以我们需要某些特定的函数来进行缩放比如：robustscaler .__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()  # 对数据进行缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 首先对训练数据进行缩放，根据对训练数据拟合的结果然后再对测试数据进行缩放\n",
    "\n",
    "n_train=train.shape[...] #获得有多少条训练数据\n",
    "\n",
    "X = data_pipe[:n_train] # 获得训练数据\n",
    "test_X = data_pipe[...:] # 获得测试数据\n",
    "y= train.SalePrice\n",
    "\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "y_log = np.log(train.SalePrice) # 对于 y 进行人为定义log 进行缩放 \n",
    "test_X_scaled = scaler.transform(...) # 对测试数据进行缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __上面的特征工程还不够，所以我们需要更多.__   \n",
    "+ __组合不同的特征通常是一个好方法，但我们不知道应该选择什么特征。 幸运的是，有些模型可以提供特征选择，这里我使用 Lasso，但你可以自由选择 Ridge、RandomForest 或 GradientBoostingTree.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一般可以先构造线性模型，分析权重来判断特征的重要性\n",
    "\n",
    "lasso=Lasso(alpha=0.001)\n",
    "lasso.fit(..., y_log)  # 用lasso 拟合 缩放后的训练数据 和 对应的 y 值 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=data_pipe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI_lasso.sort_values(\"Feature Importance\",ascending=False)  # 利用Lasso计算出一个特征重要性的分数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FI_lasso[FI_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()  ### 更直观的可视化一下"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __基于上述的可视化，我们还可以向管道中添加一些特征.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 根据特征的重要性，可以通过加减乘等方法构建新的特征\n",
    "# 我也把参考的构建方案放在了代码的最后\n",
    "\n",
    "class add_feature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,additional=1):\n",
    "        self.additional = additional\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        if self.additional==1:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "        else:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "                       \n",
    "            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n",
    "            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n",
    "            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "\n",
    "            ## here you add your new feature\n",
    "\n",
    "    \n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __By using a pipeline, you can quickily experiment different feature combinations.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 再一次完善我们的 pipeline\n",
    "pipe = Pipeline([\n",
    "    ('labenc', labelenc()),\n",
    "    ('add_feature', add_feature(additional=2)),\n",
    "    ('skew_dummies', skew_dummies(skew=1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 思考： 我们设计的这些特征（原始的和新增加的）是否存在着冗余呢？ 这些冗余（特征高度相关，导致了多重共线性）可能不利于模型更好的拟合 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PCA 可以解除这些特征的关联！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __在PCA中使用与原始数据中大致相同的维度.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipe = pipe.fit_transform(full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape, data_pipe.shape, full_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 想想为什么特征维度也不一样，是什么导致的呢？和data_pipe的特征维度之间又有哪些不同呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 类似的，对数据进行缩放\n",
    "n_train=train.shape[0]\n",
    "X = full_pipe[:...]\n",
    "test_X = full_pipe[n_train:]\n",
    "y= train.SalePrice\n",
    "\n",
    "X_scaled = scaler.fit(X).transform(X)\n",
    "y_log = np.log(train.SalePrice)\n",
    "test_X_scaled = scaler.transform(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=410)  # pca 降维，去掉特征间的冗余"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled=pca.fit_transform(...)\n",
    "test_X_scaled = pca.transform(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape, test_X_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cross validation strategy 定义一个交叉验证的策略\n",
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=...))  # 5折交叉验证\n",
    "    return rmse\n",
    "\n",
    "## 五折交叉验证： 把数据平均分成5等份，每次实验拿一份做测试，其余用做训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __我们选择了 13 个模型并使用 5 折交叉验证来评估这些模型.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models include:\n",
    "\n",
    "+ LinearRegression (LinearRegression)\n",
    "+ Ridge (Ridge)\n",
    "+ Lasso (Lasso)\n",
    "+ Random Forrest (RandomForestRegressor)\n",
    "+ Gradient Boosting Tree (GradientBoostingRegressor)\n",
    "+ Support Vector Regression (SVR)\n",
    "+ Linear Support Vector Regression (LinearSVR)\n",
    "+ ElasticNet (ElasticNet)\n",
    "+ Stochastic Gradient Descent (SGDRegressor)\n",
    "+ BayesianRidge (BayesianRidge)\n",
    "+ KernelRidge (KernelRidge)\n",
    "+ ExtraTreesRegressor (ExtraTreesRegressor)\n",
    "+ XgBoost (XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对这些模型进行初始化\n",
    "models = [LinearRegression(),Ridge(), ...(alpha=0.01,max_iter=10000), ...(), ...(), ...(), ...(),\n",
    "          ...(alpha=0.001,max_iter=10000), ...(max_iter=1000,tol=1e-3), ...(), ...(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n",
    "          ...(), ...()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\"]\n",
    "for name, model in zip(names, models):\n",
    "    score = rmse_cv(model, X_scaled, y_log)\n",
    "    print(\"{}: {:.6f}, {:.4f}\".format(name,score.mean(),score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __接下来我们进行一些超参数调整。 首先定义一个grid search方法.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 网格搜索 可以简单理解为：对超参数空间进行等分或者不等分，遍历所有可能的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid():\n",
    "    def __init__(self,model):\n",
    "        self.model = model\n",
    "    \n",
    "    def grid_get(self,X,y,param_grid):\n",
    "        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n",
    "        grid_search.fit(X,y)\n",
    "        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n",
    "        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n",
    "        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid(Lasso()).grid_get(X_scaled, ..., {'alpha': [0.0004,0.0005,0.0007,0.0006,0.0009,0.0008],'max_iter':[10000]})\n",
    "\n",
    "## [0.0004,0.0005,0.0007,0.0006,0.0009,0.0008] 就是需要搜索的超参数，下面同理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...(...()).grid_get(..., y_log,{'alpha':[35,40,45,50,55,60,65,70,80,90]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid(SVR()).grid_get(..., ..., {'C':[11,12,13,14,15],'kernel':[\"rbf\"],\"gamma\":[0.0003,0.0004],\"epsilon\":[0.008,0.009]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'alpha':[0.2,0.3,0.4,0.5], 'kernel':[\"polynomial\"], 'degree':[3],'coef0':[0.8,1,1.2]}\n",
    "grid(KernelRidge()).grid_get(X_scaled, y_log, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid(...()).grid_get(X_scaled,y_log,{'alpha':[0.0005,0.0008,0.004,0.005],'l1_ratio':[0.08,0.1,0.3,0.5,0.7],'max_iter':[10000]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 对模型进行集成。\n",
    "## 我们将多个表现较好的模型进行集成，这样可以进一步的提升性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __根据权重对基础模型进行平均.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageWeight(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self,mod,weight):\n",
    "        self.mod = mod\n",
    "        self.weight = weight\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.models_ = [clone(x) for x in self.mod]\n",
    "        for model in self.models_:\n",
    "            model.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        w = list()\n",
    "        pred = np.array([model.predict(X) for model in self.models_])\n",
    "        # for every data point, single model prediction times weight, then add them together\n",
    "        for data in range(pred.shape[1]):\n",
    "            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n",
    "            w.append(np.sum(single))\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(alpha=0.0005,max_iter=10000)\n",
    "ridge = Ridge(alpha=60)\n",
    "svr = SVR(gamma= 0.0004,kernel='rbf',C=13,epsilon=0.009)\n",
    "ker = KernelRidge(alpha=0.2 ,kernel='polynomial',degree=3 , coef0=0.8)\n",
    "ela = ElasticNet(alpha=0.005,l1_ratio=0.08,max_iter=10000)\n",
    "bay = BayesianRidge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign weights based on their gridsearch score\n",
    "# 根据他们的网格搜索 给出的测试分数 分配权重\n",
    "# 我也把参考权重放在代码的最后，鼓励大家自己去探索，知其然，知其所以然\n",
    "w1 = ...\n",
    "w2 = ...\n",
    "w3 = ...\n",
    "w4 = ...\n",
    "w5 = ...\n",
    "w6 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_avg = AverageWeight(mod = [..., ..., ..., ..., ..., ...],weight=[w1,w2,w3,w4,w5,w6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv(weight_avg,X_scaled,y_log),  rmse_cv(weight_avg,X_scaled,y_log).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __但是如果我们只对两个最好的模型进行平均，我们会获得更好的交叉验证分数.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_avg = AverageWeight(mod = [..., ...],weight=[0.5,0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_cv(weight_avg,X_scaled,y_log),  rmse_cv(weight_avg,X_scaled,y_log).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 可见在对模型的预测结果进行加权平均的时候，选择什么模型作为我们的基础模型很重要！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __除了正常的堆叠外，还添加了“get_oof”方法，可将堆叠生成的特征与原始特征结合起来.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stacking(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self,mod,meta_model):\n",
    "        self.mod = mod\n",
    "        self.meta_model = meta_model\n",
    "        self.kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.saved_model = [list() for i in self.mod]\n",
    "        oof_train = np.zeros((X.shape[0], len(self.mod)))\n",
    "        \n",
    "        for i,model in enumerate(self.mod):\n",
    "            for train_index, val_index in self.kf.split(X,y):\n",
    "                renew_model = clone(model)\n",
    "                renew_model.fit(X[train_index], y[train_index])\n",
    "                self.saved_model[i].append(renew_model)\n",
    "                oof_train[val_index,i] = renew_model.predict(X[val_index])\n",
    "        \n",
    "        self.meta_model.fit(oof_train,y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        whole_test = np.column_stack([np.column_stack(model.predict(X) for model in single_model).mean(axis=1) \n",
    "                                      for single_model in self.saved_model]) \n",
    "        return self.meta_model.predict(whole_test)\n",
    "    \n",
    "    def get_oof(self,X,y,test_X):\n",
    "        oof = np.zeros((X.shape[0],len(self.mod)))\n",
    "        test_single = np.zeros((test_X.shape[0],5))\n",
    "        test_mean = np.zeros((test_X.shape[0],len(self.mod)))\n",
    "        for i,model in enumerate(self.mod):\n",
    "            for j, (train_index,val_index) in enumerate(self.kf.split(X,y)):\n",
    "                clone_model = clone(model)\n",
    "                clone_model.fit(X[train_index],y[train_index])\n",
    "                oof[val_index,i] = clone_model.predict(X[val_index])\n",
    "                test_single[:,j] = clone_model.predict(test_X)\n",
    "            test_mean[:,i] = test_single.mean(axis=1)\n",
    "        return oof, test_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __运行这个方法有点慢，因为这个过程很复杂. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must do imputer first, otherwise stacking won't work.\n",
    "# 用于完成缺失值的插补转换器\n",
    "a = Imputer().fit_transform(X_scaled)\n",
    "b = Imputer().fit_transform(y_log.values.reshape(-1,1)).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_model = stacking(mod=[lasso,ridge, ..., ..., ..., ...], meta_model=ker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_cv(stack_model,a,b))\n",
    "print(rmse_cv(stack_model,a,b).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __接下来我们提取stacking生成的特征，然后与原始特征结合.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stack, X_test_stack = stack_model.get_oof(a,b,test_X_scaled)  # test_X_scale 的 也需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stack.shape, a.shape  # 分析 shape 变化的状态轨迹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_add = np.hstack((..., ...)) # 和训练集的原始特征在特征维度拼接 注意 原始特征在前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_add = np.hstack((..., ...)) # 和测试集的原始特征在特征维度拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_add.shape, X_test_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rmse_cv(stack_model,X_train_add,b))\n",
    "print(rmse_cv(stack_model,X_train_add,b).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ __甚至可以在获得“X_train_stack”后对您的元模型进行参数调优，或者结合原始特征进行调参, 可以改进的地方还有很多, 有兴趣的同学可以自行探索。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 参考映射函数\n",
    "\n",
    "def map_values():\n",
    "    full[\"oMSSubClass\"] = full.MSSubClass.map({'180':1, \n",
    "                                        '30':2, '45':2, \n",
    "                                        '190':3, '50':3, '90':3, \n",
    "                                        '85':4, '40':4, '160':4, \n",
    "                                        '70':5, '20':5, '75':5, '80':5, '150':5,\n",
    "                                        '120': 6, '60':6})\n",
    "    \n",
    "    full[\"oMSZoning\"] = full.MSZoning.map({'C (all)':1, 'RH':2, 'RM':2, 'RL':3, 'FV':4})\n",
    "    \n",
    "    full[\"oNeighborhood\"] = full.Neighborhood.map({'MeadowV':1,\n",
    "                                               'IDOTRR':2, 'BrDale':2,\n",
    "                                               'OldTown':3, 'Edwards':3, 'BrkSide':3,\n",
    "                                               'Sawyer':4, 'Blueste':4, 'SWISU':4, 'NAmes':4,\n",
    "                                               'NPkVill':5, 'Mitchel':5,\n",
    "                                               'SawyerW':6, 'Gilbert':6, 'NWAmes':6,\n",
    "                                               'Blmngtn':7, 'CollgCr':7, 'ClearCr':7, 'Crawfor':7,\n",
    "                                               'Veenker':8, 'Somerst':8, 'Timber':8,\n",
    "                                               'StoneBr':9,\n",
    "                                               'NoRidge':10, 'NridgHt':10})\n",
    "    \n",
    "    full[\"oCondition1\"] = full.Condition1.map({'Artery':1,\n",
    "                                           'Feedr':2, 'RRAe':2,\n",
    "                                           'Norm':3, 'RRAn':3,\n",
    "                                           'PosN':4, 'RRNe':4,\n",
    "                                           'PosA':5 ,'RRNn':5})\n",
    "    \n",
    "    full[\"oBldgType\"] = full.BldgType.map({'2fmCon':1, 'Duplex':1, 'Twnhs':1, '1Fam':2, 'TwnhsE':2})\n",
    "    \n",
    "    full[\"oHouseStyle\"] = full.HouseStyle.map({'1.5Unf':1, \n",
    "                                           '1.5Fin':2, '2.5Unf':2, 'SFoyer':2, \n",
    "                                           '1Story':3, 'SLvl':3,\n",
    "                                           '2Story':4, '2.5Fin':4})\n",
    "    \n",
    "    full[\"oExterior1st\"] = full.Exterior1st.map({'BrkComm':1,\n",
    "                                             'AsphShn':2, 'CBlock':2, 'AsbShng':2,\n",
    "                                             'WdShing':3, 'Wd Sdng':3, 'MetalSd':3, 'Stucco':3, 'HdBoard':3,\n",
    "                                             'BrkFace':4, 'Plywood':4,\n",
    "                                             'VinylSd':5,\n",
    "                                             'CemntBd':6,\n",
    "                                             'Stone':7, 'ImStucc':7})\n",
    "    \n",
    "    full[\"oMasVnrType\"] = full.MasVnrType.map({'BrkCmn':1, 'None':1, 'BrkFace':2, 'Stone':3})\n",
    "    \n",
    "    full[\"oExterQual\"] = full.ExterQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "    \n",
    "    full[\"oFoundation\"] = full.Foundation.map({'Slab':1, \n",
    "                                           'BrkTil':2, 'CBlock':2, 'Stone':2,\n",
    "                                           'Wood':3, 'PConc':4})\n",
    "    \n",
    "    full[\"oBsmtQual\"] = full.BsmtQual.map({'Fa':2, 'None':1, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oBsmtExposure\"] = full.BsmtExposure.map({'None':1, 'No':2, 'Av':3, 'Mn':3, 'Gd':4})\n",
    "    \n",
    "    full[\"oHeating\"] = full.Heating.map({'Floor':1, 'Grav':1, 'Wall':2, 'OthW':3, 'GasW':4, 'GasA':5})\n",
    "    \n",
    "    full[\"oHeatingQC\"] = full.HeatingQC.map({'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oKitchenQual\"] = full.KitchenQual.map({'Fa':1, 'TA':2, 'Gd':3, 'Ex':4})\n",
    "    \n",
    "    full[\"oFunctional\"] = full.Functional.map({'Maj2':1, 'Maj1':2, 'Min1':2, 'Min2':2, 'Mod':2, 'Sev':2, 'Typ':3})\n",
    "    \n",
    "    full[\"oFireplaceQu\"] = full.FireplaceQu.map({'None':1, 'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5})\n",
    "    \n",
    "    full[\"oGarageType\"] = full.GarageType.map({'CarPort':1, 'None':1,\n",
    "                                           'Detchd':2,\n",
    "                                           '2Types':3, 'Basment':3,\n",
    "                                           'Attchd':4, 'BuiltIn':5})\n",
    "    \n",
    "    full[\"oGarageFinish\"] = full.GarageFinish.map({'None':1, 'Unf':2, 'RFn':3, 'Fin':4})\n",
    "    \n",
    "    full[\"oPavedDrive\"] = full.PavedDrive.map({'N':1, 'P':2, 'Y':3})\n",
    "    \n",
    "    full[\"oSaleType\"] = full.SaleType.map({'COD':1, 'ConLD':1, 'ConLI':1, 'ConLw':1, 'Oth':1, 'WD':1,\n",
    "                                       'CWD':2, 'Con':3, 'New':3})\n",
    "    \n",
    "    full[\"oSaleCondition\"] = full.SaleCondition.map({'AdjLand':1, 'Abnorml':2, 'Alloca':2, 'Family':2, 'Normal':3, 'Partial':4})            \n",
    "                \n",
    "                        \n",
    "                        \n",
    "    \n",
    "    return \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class add_feature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,additional=1):\n",
    "        self.additional = additional\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        if self.additional==1:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "        else:\n",
    "            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n",
    "            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n",
    "            \n",
    "            X[\"+_TotalHouse_OverallQual\"] = X[\"TotalHouse\"] * X[\"OverallQual\"]\n",
    "            X[\"+_GrLivArea_OverallQual\"] = X[\"GrLivArea\"] * X[\"OverallQual\"]\n",
    "            X[\"+_oMSZoning_TotalHouse\"] = X[\"oMSZoning\"] * X[\"TotalHouse\"]\n",
    "            X[\"+_oMSZoning_OverallQual\"] = X[\"oMSZoning\"] + X[\"OverallQual\"]\n",
    "            X[\"+_oMSZoning_YearBuilt\"] = X[\"oMSZoning\"] + X[\"YearBuilt\"]\n",
    "            X[\"+_oNeighborhood_TotalHouse\"] = X[\"oNeighborhood\"] * X[\"TotalHouse\"]\n",
    "            X[\"+_oNeighborhood_OverallQual\"] = X[\"oNeighborhood\"] + X[\"OverallQual\"]\n",
    "            X[\"+_oNeighborhood_YearBuilt\"] = X[\"oNeighborhood\"] + X[\"YearBuilt\"]\n",
    "            X[\"+_BsmtFinSF1_OverallQual\"] = X[\"BsmtFinSF1\"] * X[\"OverallQual\"]\n",
    "            \n",
    "            X[\"-_oFunctional_TotalHouse\"] = X[\"oFunctional\"] * X[\"TotalHouse\"]\n",
    "            X[\"-_oFunctional_OverallQual\"] = X[\"oFunctional\"] + X[\"OverallQual\"]\n",
    "            X[\"-_LotArea_OverallQual\"] = X[\"LotArea\"] * X[\"OverallQual\"]\n",
    "            X[\"-_TotalHouse_LotArea\"] = X[\"TotalHouse\"] + X[\"LotArea\"]\n",
    "            X[\"-_oCondition1_TotalHouse\"] = X[\"oCondition1\"] * X[\"TotalHouse\"]\n",
    "            X[\"-_oCondition1_OverallQual\"] = X[\"oCondition1\"] + X[\"OverallQual\"]\n",
    "            \n",
    "           \n",
    "            X[\"Bsmt\"] = X[\"BsmtFinSF1\"] + X[\"BsmtFinSF2\"] + X[\"BsmtUnfSF\"]\n",
    "            X[\"Rooms\"] = X[\"FullBath\"]+X[\"TotRmsAbvGrd\"]\n",
    "            X[\"PorchArea\"] = X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "            X[\"TotalPlace\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"] + X[\"OpenPorchSF\"]+X[\"EnclosedPorch\"]+X[\"3SsnPorch\"]+X[\"ScreenPorch\"]\n",
    "\n",
    "    \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.02\n",
    "w2 = 0.2\n",
    "w3 = 0.25\n",
    "w4 = 0.3\n",
    "w5 = 0.03\n",
    "w6 = 0.2"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
